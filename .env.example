# Ollama Local AI Configuration (100% privacy - no external API calls)
# Install Ollama from https://ollama.com
# Download models: ollama pull llama3.1:8b && ollama pull qwen2.5-vl:7b
OLLAMA_HOST=http://localhost:11434

# Primary model for text/search tasks
OLLAMA_MODEL=llama3.1:8b

# Vision model for OCR and image understanding
OLLAMA_VISION_MODEL=qwen2.5-vl:7b

# LLM Parameters
OLLAMA_TEMPERATURE=0.3
OLLAMA_MAX_TOKENS=2000
OLLAMA_TIMEOUT=120

# GPU Acceleration (optional, for faster inference)
# Set to number of layers to offload to GPU, or -1 for all layers
OLLAMA_GPU_LAYERS=-1

# Application Settings
APP_NAME=FindingExcellence_PRO
APP_VERSION=2.0.0
APP_URL=https://findingexcellence.app

# Search Settings
DEFAULT_SEARCH_FOLDERS=C:\Users\Desktop,C:\Users\Downloads
MAX_WORKERS=4

# Logging
LOG_LEVEL=INFO
LOG_FILE=finding_excellence.log
