# Ollama Local AI Configuration (100% privacy - no external API calls)
# Install Ollama from https://ollama.com
# Download models: ollama pull qwen3:4b && ollama pull deepseek-ocr
OLLAMA_HOST=http://localhost:11434

# December 2025 Optimized Models (faster, smaller, same quality)
# Primary model for text/search tasks - Qwen3-4B matches 72B quality at 4-8x speed
OLLAMA_MODEL=qwen3:4b

# Vision model for OCR and image understanding - DeepSeek-OCR 97% accuracy
OLLAMA_VISION_MODEL=deepseek-ocr

# Fast reasoning model for quick tasks
OLLAMA_FAST_MODEL=deepseek-r1:1.5b

# LLM Parameters
OLLAMA_TEMPERATURE=0.3
OLLAMA_MAX_TOKENS=2000
OLLAMA_TIMEOUT=120

# December 2025 Speed Optimizations
# Keep models loaded in memory (eliminates cold-start delay)
# Values: -1 (forever), 0 (unload immediately), 5m (5 minutes), 1h (1 hour)
OLLAMA_KEEP_ALIVE=-1

# Context window size (smaller = faster, default 2048)
# Reduce for speed, increase for longer documents
OLLAMA_CONTEXT_SIZE=2048

# Number of threads for CPU inference (default: half of CPU cores)
# OLLAMA_NUM_THREADS=4

# GPU Acceleration (optional, for faster inference)
# Set to number of layers to offload to GPU, or -1 for all layers
OLLAMA_GPU_LAYERS=-1

# Application Settings
APP_NAME=FindingExcellence_PRO
APP_VERSION=2.0.0
APP_URL=https://findingexcellence.app

# Search Settings
DEFAULT_SEARCH_FOLDERS=C:\Users\Desktop,C:\Users\Downloads
MAX_WORKERS=4

# Logging
LOG_LEVEL=INFO
LOG_FILE=finding_excellence.log
